Project Root: .

---- TREE ----
.
â”œâ”€â”€ app
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ config_loader.py
â”‚Â Â  â”œâ”€â”€ embedding_utils.py
â”‚Â Â  â”œâ”€â”€ generate_cover.py
â”‚Â Â  â”œâ”€â”€ generate_resume.py
â”‚Â Â  â”œâ”€â”€ services
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cover_service.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ resume_service.py
â”‚Â Â  â””â”€â”€ workflow_runner.py
â”œâ”€â”€ config.yaml
â”œâ”€â”€ context
â”‚Â Â  â”œâ”€â”€ CoverLetters
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ CL_1.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ CL_10.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ CL_2.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ CL_3.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ CL_4.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ CL_5.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ CL_6.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ CL_7.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ CL_8.md
â”‚Â Â  â”‚Â Â  â””â”€â”€ CL_9.md
â”‚Â Â  â””â”€â”€ Resume_Full.md
â”œâ”€â”€ context.txt
â”œâ”€â”€ main.py
â”œâ”€â”€ outputs
â”œâ”€â”€ prompts
â”‚Â Â  â”œâ”€â”€ prompt_coverletter.txt
â”‚Â Â  â””â”€â”€ prompt_resume.txt
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ tests
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_config_loader.py
    â”œâ”€â”€ test_cover_service.py
    â”œâ”€â”€ test_embedding_utils.py
    â”œâ”€â”€ test_resume_service.py
    â””â”€â”€ test_workflow_runner.py

8 directories, 34 files

---- app/__init__.py ----

---- app/config_loader.py ----
import yaml
import os


def load_config(path: str = "config.yaml") -> dict:
    with open(path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    print(cfg)
    base = os.path.dirname(os.path.abspath(path))
    for key, value in cfg.items():
        if isinstance(value, str):
            cfg[key] = os.path.normpath(os.path.join(base, value))
    return cfg

---- app/embedding_utils.py ----
import faiss
import numpy as np
import glob
import os
from langchain_openai import OpenAIEmbeddings


class FAISSIndex:
    def __init__(self, index, texts):
        self.index = index
        self.texts = texts

    def query(self, query_text, top_k=3):
        emb = OpenAIEmbeddings()
        qv = np.array([emb.embed_query(query_text)], dtype=np.float32)
        D, I = self.index.search(qv, top_k)
        return [self.texts[i] for i in I[0] if i < len(self.texts)]


def build_faiss_index(coverletters_dir):
    emb = OpenAIEmbeddings()
    files = glob.glob(os.path.join(coverletters_dir, "*.md"))
    texts = [open(f).read() for f in files]
    if not texts:
        raise RuntimeError("No cover letter templates found in context/CoverLetters")
    vecs = emb.embed_documents(texts)
    mat = np.array(vecs, dtype=np.float32)
    index = faiss.IndexFlatL2(mat.shape[1])
    index.add(mat)
    return FAISSIndex(index, texts)

---- app/generate_cover.py ----
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate


def generate_cover_letter(
    job_title: str,
    company: str,
    job_description: str,
    resume_path: str,
    index,
    output_path: str,
):
    with open(resume_path, "r", encoding="utf-8") as f:
        tailored_resume = f.read()
    with open("app/prompts/prompt_coverletter.txt", "r", encoding="utf-8") as f:
        prompt_template = f.read()

    retrieved_contexts = index.query(job_description, top_k=3)
    combined_context = "\n\n".join(retrieved_contexts)

    prompt = PromptTemplate.from_template(prompt_template)
    llm = ChatOpenAI(model="gpt-4.1", temperature=0)
    chain = prompt | llm

    result = chain.invoke(
        {
            "job_title": job_title,
            "company": company,
            "job_description": job_description,
            "tailored_resume": tailored_resume,
            "retrieved_templates": combined_context,
        }
    )

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(result.content)

---- app/generate_resume.py ----
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate


def generate_resume(
    job_title: str, company: str, job_description: str, output_path: str
):
    with open("context/Resume_Full.md", "r", encoding="utf-8") as f:
        resume_full = f.read()
    with open("app/prompts/prompt_resume.txt", "r", encoding="utf-8") as f:
        prompt_template = f.read()

    prompt = PromptTemplate.from_template(prompt_template)
    llm = ChatOpenAI(model="gpt-4.1", temperature=0)
    chain = prompt | llm

    result = chain.invoke(
        {
            "job_title": job_title,
            "company": company,
            "job_description": job_description,
            "resume_full": resume_full,
        }
    )

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(result.content)

---- app/services/__init__.py ----

---- app/services/cover_service.py ----
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate


class CoverLetterService:
    def __init__(self, prompt_path: str):
        with open(prompt_path, "r", encoding="utf-8") as f:
            template_str = f.read()
        self.prompt = PromptTemplate.from_template(template_str)
        self.llm = ChatOpenAI(model="gpt-4.1", temperature=0)

    def generate(
        self,
        job_title: str,
        company: str,
        job_description: str,
        tailored_resume: str,
        retrieved_templates: str,
    ) -> str:
        chain = self.prompt | self.llm
        result = chain.invoke(
            {
                "job_title": job_title,
                "company": company,
                "job_description": job_description,
                "tailored_resume": tailored_resume,
                "retrieved_templates": retrieved_templates,
            }
        )
        return result.content

---- app/services/resume_service.py ----
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate


class ResumeService:
    def __init__(self, prompt_path: str):
        with open(prompt_path, "r", encoding="utf-8") as f:
            template_str = f.read()
        self.prompt = PromptTemplate.from_template(template_str)
        self.llm = ChatOpenAI(model="gpt-4.1", temperature=0)

    def generate(
        self, job_title: str, company: str, job_description: str, resume_full: str
    ) -> str:
        chain = self.prompt | self.llm
        result = chain.invoke(
            {
                "job_title": job_title,
                "company": company,
                "job_description": job_description,
                "resume_full": resume_full,
            }
        )
        return result.content

---- app/workflow_runner.py ----
import os
import uuid
import re
from datetime import datetime


class WorkflowRunner:
    def __init__(
        self,
        resume_service,
        cover_service,
        index,
        resume_context_path: str,
        output_dir: str = "outputs",
    ):
        self.resume_service = resume_service
        self.cover_service = cover_service
        self.index = index
        self.resume_context_path = resume_context_path
        self.output_dir = output_dir

    def _clean_name(self, name: str) -> str:
        return re.sub(r"[^a-z0-9]+", "_", name.strip().lower()).strip("_")

    def run(self, job_title: str, company: str, job_description: str) -> dict:
        date_str = datetime.now().strftime("%m_%d_%Y")
        job_clean = self._clean_name(job_title)
        company_clean = self._clean_name(company)
        run_id = str(uuid.uuid4())[:8]

        run_dir = os.path.join(self.output_dir, f"{date_str}_{job_clean}_{run_id}")
        os.makedirs(run_dir, exist_ok=True)

        # Read full resume context
        with open(self.resume_context_path, "r", encoding="utf-8") as f:
            resume_full = f.read()

        # Generate tailored resume
        tailored_resume = self.resume_service.generate(
            job_title, company, job_description, resume_full
        )
        resume_filename = f"resume_{job_clean}_{company_clean}_{date_str}.md"
        resume_path = os.path.join(run_dir, resume_filename)
        with open(resume_path, "w", encoding="utf-8") as f:
            f.write(tailored_resume)

        # Generate tailored cover letter
        retrieved_contexts = self.index.query(job_description, top_k=3)
        combined_context = "\n\n".join(retrieved_contexts)
        tailored_cover = self.cover_service.generate(
            job_title, company, job_description, tailored_resume, combined_context
        )
        cover_filename = f"cover_letter_{job_clean}_{company_clean}_{date_str}.md"
        cover_path = os.path.join(run_dir, cover_filename)
        with open(cover_path, "w", encoding="utf-8") as f:
            f.write(tailored_cover)

        return {
            "output_dir": run_dir,
            "resume_path": resume_path,
            "cover_path": cover_path,
        }

---- config.yaml ----
# Path to the resume prompt template
resume_prompt: prompts/prompt_resume.txt

# Path to the cover letter prompt template
coverletter_prompt: prompts/prompt_coverletter.txt

# Path to the full resume context
resume_context: context/Resume_Full.md

# Folder containing example cover letters for similarity search
cover_letter_context_folder: context/CoverLetters

---- main.py ----
import streamlit as st
from app.config_loader import load_config
from app.embedding_utils import build_faiss_index
from app.services.resume_service import ResumeService
from app.services.cover_service import CoverLetterService
from app.workflow_runner import WorkflowRunner

# Load configuration
cfg = load_config()

# Build services and dependencies
resume_service = ResumeService(cfg["resume_prompt"])
cover_service = CoverLetterService(cfg["coverletter_prompt"])
index = build_faiss_index(cfg["cover_letter_context_folder"])

# Inject dependencies into the workflow runner
workflow = WorkflowRunner(
    resume_service=resume_service,
    cover_service=cover_service,
    index=index,
    resume_context_path=cfg["resume_context"],
)

# Streamlit UI
st.title("ApplyAI")

job_title = st.text_input("Job Title")
company = st.text_input("Company")
job_description = st.text_area("Job Description", height=300)

if st.button("Generate"):
    if not all([job_title, company, job_description]):
        st.error("All fields are required.")
    else:
        with st.spinner("ðŸ“ Generating tailored documents..."):
            result = workflow.run(job_title, company, job_description)

        st.success(f"âœ… Generation complete! Files saved in `{result['output_dir']}`")

        st.download_button(
            f"â¬‡ï¸ Download Tailored Resume ({job_title})",
            data=open(result["resume_path"], "rb").read(),
            file_name=result["resume_path"].split("/")[-1],
        )
        st.download_button(
            f"â¬‡ï¸ Download Tailored Cover Letter ({job_title})",
            data=open(result["cover_path"], "rb").read(),
            file_name=result["cover_path"].split("/")[-1],
        )

---- tests/__init__.py ----

---- tests/test_config_loader.py ----
import io
import os
import tempfile
import textwrap
from app.config_loader import load_config

def test_load_config_resolves_paths(tmp_path):
    # Create a temporary config.yaml with relative paths
    yaml_content = textwrap.dedent("""
        resume_prompt: prompts/prompt_resume.txt
        coverletter_prompt: prompts/prompt_coverletter.txt
        resume_context: context/Resume_Full.md
        cover_letter_context_folder: context/CoverLetters
    """)
    cfg_file = tmp_path / "config_test.yaml"
    cfg_file.write_text(yaml_content, encoding="utf-8")

    # Create dummy dirs/files to ensure normalization works (not strictly necessary)
    (tmp_path / "prompts").mkdir()
    (tmp_path / "context").mkdir()
    (tmp_path / "context" / "CoverLetters").mkdir()
    (tmp_path / "prompts" / "prompt_resume.txt").write_text("dummy")
    (tmp_path / "prompts" / "prompt_coverletter.txt").write_text("dummy")
    (tmp_path / "context" / "Resume_Full.md").write_text("dummy")

    # Run loader on this temp config
    cfg = load_config(str(cfg_file))

    # Assert keys exist
    assert "resume_prompt" in cfg
    assert "coverletter_prompt" in cfg
    assert "resume_context" in cfg
    assert "cover_letter_context_folder" in cfg

    # Assert the values are absolute paths (normalized)
    for key, path in cfg.items():
        assert os.path.isabs(path), f"{key} is not absolute: {path}"
        # optional: check the path points under tmp_path
        assert str(tmp_path) in path

---- tests/test_cover_service.py ----
import io
import builtins
import pytest
from app.services.cover_service import CoverLetterService

class DummyPrompt:
    def __or__(self, other):
        return DummyChain()

class DummyLLM:
    pass

class DummyChain:
    def invoke(self, inputs):
        return type("Obj", (object,), {
            "content": f"{inputs['job_title']}->{inputs['company']}"
        })

@pytest.fixture(autouse=True)
def patch_prompt_and_llm(monkeypatch):
    monkeypatch.setattr(
        "app.services.cover_service.PromptTemplate.from_template",
        lambda _: DummyPrompt()
    )
    monkeypatch.setattr(
        "app.services.cover_service.ChatOpenAI",
        lambda **kwargs: DummyLLM()
    )

    def fake_open(path, mode="r", encoding=None):
        if "prompt" in path:
            return io.StringIO("DUMMY PROMPT CONTENT")
        return builtins.open(path, mode, encoding=encoding)
    monkeypatch.setattr("builtins.open", fake_open)

def test_generate_builds_prompt_correctly():
    service = CoverLetterService("any/prompt/path.txt")
    result = service.generate("Engineer", "AcmeCorp", "JD", "ResumeTxt", "Examples")
    assert "Engineer" in result
    assert "AcmeCorp" in result

---- tests/test_embedding_utils.py ----
import numpy as np
import tempfile
import os
import pytest
from app.embedding_utils import build_faiss_index

class DummyEmbeddings:
    def embed_query(self, text):
        return [1.0, 2.0, 3.0]  # fixed vector
    def embed_documents(self, texts):
        return [[0.1, 0.2, 0.3] for _ in texts]

@pytest.fixture(autouse=True)
def patch_openai_embeddings(monkeypatch):
    monkeypatch.setattr("app.embedding_utils.OpenAIEmbeddings", lambda : DummyEmbeddings())

def test_build_faiss_index_and_query(tmp_path):
    # Create dummy .md files
    (tmp_path / "a.md").write_text("doc A")
    (tmp_path / "b.md").write_text("doc B")

    # Build index
    index = build_faiss_index(str(tmp_path))
    assert len(index.texts) == 2

    # Query
    results = index.query("some query", top_k=1)
    assert len(results) == 1
    assert any("doc A" in r or "doc B" in r for r in results)

---- tests/test_resume_service.py ----
import io
import builtins
import pytest
from app.services.resume_service import ResumeService

# Dummy classes
class DummyPrompt:
    def __or__(self, other):
        return DummyChain()

class DummyLLM:
    pass

class DummyChain:
    def invoke(self, inputs):
        return type("Obj", (object,), {
            "content": f"{inputs['job_title']}|{inputs['company']}"
        })

@pytest.fixture(autouse=True)
def patch_prompt_and_llm(monkeypatch):
    # Patch the prompt and LLM classes to avoid real init logic
    monkeypatch.setattr(
        "app.services.resume_service.PromptTemplate.from_template",
        lambda _: DummyPrompt()
    )
    monkeypatch.setattr(
        "app.services.resume_service.ChatOpenAI",
        lambda **kwargs: DummyLLM()
    )

    # Patch open() only for prompt_path, return a dummy file-like object
    def fake_open(path, mode="r", encoding=None):
        if "prompt" in path:
            return io.StringIO("DUMMY PROMPT CONTENT")
        return builtins.open(path, mode, encoding=encoding)
    monkeypatch.setattr("builtins.open", fake_open)

def test_generate_builds_prompt_correctly():
    service = ResumeService("any/prompt/path.txt")  # uses patched open
    result = service.generate("Engineer", "AcmeCorp", "Some JD", "FullResume")
    assert "Engineer" in result
    assert "AcmeCorp" in result

---- tests/test_workflow_runner.py ----
# tests/test_workflow_runner.py
import os
import tempfile
from app.workflow_runner import WorkflowRunner

class DummyService:
    def __init__(self, output):
        self.output = output
    def generate(self, *args, **kwargs):
        return self.output

class DummyIndex:
    def query(self, text, top_k=3):
        return ["contextA", "contextB"]

def test_workflow_runner_creates_files(tmp_path):
    # Prepare runner with dummy services
    resume_service = DummyService("RESUME_CONTENT")
    cover_service = DummyService("COVER_CONTENT")
    index = DummyIndex()

    # Create a fake resume context
    context_path = tmp_path / "resume_full.md"
    context_path.write_text("BASE_RESUME")

    runner = WorkflowRunner(
        resume_service=resume_service,
        cover_service=cover_service,
        index=index,
        resume_context_path=str(context_path),
        output_dir=str(tmp_path)
    )

    result = runner.run("Engineer", "AcmeCorp", "Some Job Description")

    # Verify files exist
    assert os.path.exists(result["resume_path"])
    assert os.path.exists(result["cover_path"])

    # Verify file contents
    assert "RESUME_CONTENT" in open(result["resume_path"]).read()
    assert "COVER_CONTENT" in open(result["cover_path"]).read()

